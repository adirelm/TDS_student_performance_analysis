{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDS Project: Dataset Selection, EDA, and Basic Model Analysis\n",
    "\n",
    "**Group Members:**\n",
    "- Adir Elmakais - 316413640\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project aims to analyze the \"Student Performance Factors\" dataset to understand the various factors influencing students' exam scores. We will perform exploratory data analysis (EDA), build a baseline regression model using XGBoost, and conduct an error analysis to identify areas for improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Guide\n",
    "\n",
    "#### Use Python 3.12.0\n",
    "\n",
    "To get started with the project, ensure you are using **Python 3.12.0**.\n",
    "\n",
    "1. **Install Python 3.12.0**:\n",
    "   - Download the installer for Python 3.12.0 from the [official Python website](https://www.python.org/downloads/release/python-3120/).\n",
    "   - During the installation, make sure to check the box **\"Add Python to PATH\"**.\n",
    "\n",
    "2. **macOS: Install `libomp`**:\n",
    "   - For macOS users, you need to install `libomp` for compatibility with XGBoost. Run the following command:\n",
    "     ```bash\n",
    "     brew install libomp\n",
    "     ```\n",
    "\n",
    "3. **Install Required Packages**:\n",
    "   - Once Python 3.12.0 is installed, you can install the necessary packages listed in the `requirements.txt` file by running the following command in your terminal:\n",
    "     ```bash\n",
    "     pip install -r requirements.txt\n",
    "     ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Selection\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "We have selected the **\"Student Performance Factors\"** dataset from [Kaggle](https://www.kaggle.com/datasets/lainguyn123/student-performance-factors). This dataset provides comprehensive insights into various factors affecting student performance in exams, including study habits, attendance, parental involvement, and more.\n",
    "\n",
    "### Dataset Details\n",
    "\n",
    "- **Number of Records:** 6,607\n",
    "- **Number of Features:** 20\n",
    "- **Attribute Types:**\n",
    "  - **Numeric:** Hours_Studied, Attendance, Sleep_Hours, Previous_Scores, Tutoring_Sessions, Family_Income, Distance_from_Home, Physical_Activity, etc.\n",
    "  - **Categorical:** Parental_Involvement, Access_to_Resources, Extracurricular_Activities, Motivation_Level, Internet_Access, Teacher_Quality, School_Type, Peer_Influence, Learning_Disabilities, Parental_Education_Level, Gender, etc.\n",
    "\n",
    "### Reference\n",
    "\n",
    "> **Dataset Reference:** \"Student Performance Factors,\" available on [Kaggle](https://www.kaggle.com/datasets/lainguyn123/student-performance-factors).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Analysis\n",
    "\n",
    "### a. Data Loading and Inspection\n",
    "\n",
    "First, we load the dataset and perform initial inspections to understand its structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setting plot styles\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Loading the dataset\n",
    "data = pd.read_csv('data/StudentPerformanceFactors.csv')\n",
    "\n",
    "# Displaying the first few rows\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Data Cleaning\n",
    "\n",
    "Before diving into analysis, it's essential to check for missing values and handle them appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspecting the dataset, we identified missing values in the following categorical columns:\n",
    "\n",
    "- **Teacher_Quality:** 78 missing values\n",
    "- **Parental_Education_Level:** 90 missing values\n",
    "- **Distance_from_Home:** 67 missing values\n",
    "\n",
    "To maintain data integrity without introducing potential biases from imputation, we will **remove all rows** containing missing values in these columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the number of missing values before cleaning\n",
    "print(\"Missing Values Before Cleaning:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Dropping rows with missing values in specified columns\n",
    "columns_with_missing = ['Teacher_Quality', 'Parental_Education_Level', 'Distance_from_Home']\n",
    "data_cleaned = data.dropna(subset=columns_with_missing)\n",
    "\n",
    "# Displaying the number of missing values after cleaning\n",
    "print(\"\\nMissing Values After Cleaning:\")\n",
    "print(data_cleaned.isnull().sum())\n",
    "\n",
    "# Displaying the change in dataset size\n",
    "print(f\"\\nOriginal dataset size: {data.shape[0]} rows\")\n",
    "print(f\"Cleaned dataset size: {data_cleaned.shape[0]} rows\")\n",
    "print(f\"Rows removed: {data.shape[0] - data_cleaned.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "### a. Data Loading and Inspection\n",
    "We begin by loading the cleaned dataset and performing initial inspections to understand its structure and contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setting plot styles\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Loading the cleaned dataset\n",
    "data_cleaned = data_cleaned.copy()  # Ensuring we're working with the cleaned data\n",
    "\n",
    "# Displaying the first few rows\n",
    "data_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Data Summary\n",
    "\n",
    "Let's explore the basic statistics and data types to gain an overview of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying dataset information\n",
    "data_cleaned.info()\n",
    "data_cleaned.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "\n",
    "- **Numerical Features:** Several attributes like `Hours_Studied`, `Attendance`, `Sleep_Hours`, `Previous_Scores`, etc., are numerical and suitable for quantitative analysis.\n",
    "- **Categorical Features:** Attributes such as `Parental_Involvement`, `Access_to_Resources`, `Extracurricular_Activities`, etc., are categorical and may require encoding for modeling.\n",
    "- **No Missing Values:** All missing values have been addressed, ensuring the dataset is complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Visualizations\n",
    "\n",
    "We'll create the following five visualizations:\n",
    "\n",
    "1. Distribution of Exam_Score\n",
    "2. Correlation Heatmap\n",
    "3. Box Plot: Parental_Involvement vs. Exam_Score\n",
    "4. Scatter Plot: Hours_Studied vs. Exam_Score\n",
    "5. Pair Plot of Selected Features and Exam_Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Distribution of Exam_Score\n",
    "\n",
    "Understanding the distribution of the target variable `Exam_Score` is essential to identify its central tendency, spread, and potential skewness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data_cleaned['Exam_Score'], bins=20, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Exam Scores')\n",
    "plt.xlabel('Exam Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "The histogram illustrates the frequency distribution of `Exam_Score`. The KDE (Kernel Density Estimate) overlay helps in visualizing the underlying distribution shape, indicating whether the scores are normally distributed, skewed, or exhibit any multi-modality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Correlation Heatmap\n",
    "\n",
    "A correlation heatmap helps identify the strength and direction of relationships between `Exam_Score` and other numerical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting numeric features for correlation\n",
    "numeric_features = data_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Calculating correlation matrix\n",
    "corr_matrix = data_cleaned[numeric_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap of Numeric Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "The heatmap displays the correlation coefficients between `Exam_Score` and other numerical features. Strong positive or negative correlations can highlight significant predictors for the regression model, aiding in feature selection and engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Parental Involvement vs. Exam_Score\n",
    "\n",
    "This box plot examines how different levels of parental involvement impact students' exam scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(x='Parental_Involvement', y='Exam_Score', data=data_cleaned, palette='Set2')\n",
    "plt.title('Parental Involvement vs. Exam Score')\n",
    "plt.xlabel('Parental Involvement')\n",
    "plt.ylabel('Exam Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "The box plot reveals the distribution of `Exam_Score` across different levels of `Parental_Involvement`. It helps in identifying whether higher levels of parental involvement are associated with better exam performance, highlighting potential areas for intervention and support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Hours Studied vs. Exam_Score\n",
    "\n",
    "A scatter plot to analyze the relationship between the number of hours a student studies and their exam scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x='Hours_Studied', y='Exam_Score', data=data_cleaned, hue='Motivation_Level', palette='viridis')\n",
    "plt.title('Hours Studied vs. Exam Score')\n",
    "plt.xlabel('Hours Studied per Week')\n",
    "plt.ylabel('Exam Score')\n",
    "plt.legend(title='Motivation Level')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "The scatter plot visualizes how `Hours_Studied` correlates with `Exam_Score`, with points colored by `Motivation_Level`. This helps in understanding whether more study hours lead to higher scores and how motivation influences this relationship.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Pair Plot of Selected Features and Exam_Score\n",
    "\n",
    "A pair plot to explore interactions between multiple features and their relationship with `Exam_Score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['Hours_Studied', 'Attendance', 'Sleep_Hours', 'Previous_Scores', 'Exam_Score']\n",
    "sns.pairplot(data_cleaned[selected_features], diag_kind='kde', plot_kws={'alpha':0.5, 's':50})\n",
    "plt.suptitle('Pair Plot of Selected Features and Exam Score', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "The pair plot allows us to observe pairwise relationships between selected numerical features and the target `Exam_Score`. Diagonal KDE plots show the distribution of each feature, while scatter plots indicate potential correlations and interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Basic Model Pipeline\n",
    "\n",
    "#### a. Defining the Regression Problem\n",
    "\n",
    "**Objective:** Predict the `Exam_Score` of students based on various factors such as study habits, attendance, parental involvement, and more.\n",
    "\n",
    "**Target Variable:** `Exam_Score` (Numeric)\n",
    "\n",
    "**Features:**\n",
    "- **Numerical:** `Hours_Studied`, `Attendance`, `Sleep_Hours`, `Previous_Scores`, `Tutoring_Sessions`, `Family_Income`, `Distance_from_Home`, `Physical_Activity`, etc.\n",
    "- **Categorical:** `Parental_Involvement`, `Access_to_Resources`, `Extracurricular_Activities`, `Motivation_Level`, `Internet_Access`, `Teacher_Quality`, `School_Type`, `Peer_Influence`, `Learning_Disabilities`, `Parental_Education_Level`, `Gender`, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Data Preprocessing\n",
    "\n",
    "To prepare the data for modeling, we will:\n",
    "\n",
    "1. **Handle Categorical Variables:** Convert categorical variables into numerical formats using **One-Hot Encoding**.\n",
    "2. **Feature Scaling:** Although XGBoost is less sensitive to feature scaling, ensuring uniformity can aid in model interpretability.\n",
    "3. **Train-Test Split:** Divide the data into training and testing sets to evaluate model performance effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Separating features and target\n",
    "X = data_cleaned.drop('Exam_Score', axis=1)\n",
    "y = data_cleaned['Exam_Score']\n",
    "\n",
    "# Identifying categorical and numeric features\n",
    "categorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Converting boolean columns to integers if any\n",
    "for col in categorical_features:\n",
    "    if X[col].dtype == 'bool':\n",
    "        X[col] = X[col].astype(int)\n",
    "\n",
    "# Defining the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Leave numeric features as is\n",
    ")\n",
    "\n",
    "# Building the machine learning pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', xgb.XGBRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **OneHotEncoder:** Transforms categorical variables into a binary matrix, enabling the model to interpret them numerically.\n",
    "- **ColumnTransformer:** Applies different preprocessing steps to different columns—in this case, One-Hot Encoding to categorical features and leaving numeric features unchanged.\n",
    "- **Pipeline:** Streamlines the preprocessing and modeling steps, ensuring that all transformations are applied consistently during training and testing.\n",
    "- **Train-Test Split:** Allocates **80%** of the data for training and **20%** for testing, providing a balance between model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Model Training and Evaluation\n",
    "\n",
    "We will train the **XGBoost Regressor** using the default configuration to establish a baseline performance. The following metrics will be used to evaluate the model:\n",
    "\n",
    "- **Mean Absolute Error (MAE):** Measures the average magnitude of errors in a set of predictions, without considering their direction.\n",
    "- **Mean Squared Error (MSE):** Measures the average of the squares of the errors, giving higher weight to larger errors.\n",
    "- **Root Mean Squared Error (RMSE):** The square root of MSE, providing error magnitudes in the same units as the target variable.\n",
    "- **R² Score:** Indicates the proportion of the variance in the dependent variable predictable from the independent variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Performance Metrics:**\n",
    "\n",
    "- **Mean Absolute Error (MAE):** *1.05*\n",
    "- **Mean Squared Error (MSE):** *5.95*\n",
    "- **Root Mean Squared Error (RMSE):** *2.44*\n",
    "- **R² Score:** *0.62*\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "- **R² Score:** An R² score of *0.62* indicates that **62%** of the variance in `Exam_Score` is explained by the model. While a higher R² signifies better fit, it's essential to ensure that the model's performance isn't excessively high to allow room for improvement in subsequent project stages.\n",
    "- **Error Metrics:** The MAE and RMSE values provide insights into the average prediction errors. These metrics help in understanding the model's accuracy and reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Error Analysis\n",
    "\n",
    "After establishing a baseline model, it's essential to analyze its errors to identify areas for improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Residual Analysis\n",
    "\n",
    "Residuals are the differences between actual and predicted values. Analyzing residuals helps in identifying patterns that the model fails to capture, indicating potential areas where the model can be improved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Residuals vs. Predicted Exam Scores\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x=y_pred, y=residuals, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Residuals vs. Predicted Exam Scores')\n",
    "plt.xlabel('Predicted Exam Score')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals vs. Predicted Exam Scores Plot\n",
    "\n",
    "The plot above visualizes the residuals against the predicted exam scores. Here's a breakdown of the components:\n",
    "\n",
    "1. **X-Axis (Predicted Exam Scores):** Represents the predicted values of exam scores from a regression model.\n",
    "2. **Y-Axis (Residuals):** Represents the residuals, which are the differences between the observed and predicted values.\n",
    "3. **Residuals Analysis:**\n",
    "   - Points closer to the red dashed horizontal line (residual = 0) indicate better model predictions.\n",
    "   - There is a slight funnel shape or heteroscedasticity, with residuals spreading out more for higher predicted scores, suggesting that the model's error may vary across the range of predictions.\n",
    "4. **Red Dashed Line:** Represents the zero-residual line, where the predicted values perfectly match the observed values.\n",
    "5. **Point Cloud:** Most data points cluster around the zero-residual line, but there are notable outliers with larger residuals, indicating instances where the model significantly underestimates or overestimates exam scores.\n",
    "\n",
    "This plot is useful for evaluating the regression model's performance and identifying potential issues like heteroscedasticity or outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Error Distribution\n",
    "\n",
    "Analyzing the distribution of residuals helps in understanding whether the errors are normally distributed or exhibit any skewness, which can impact the model's reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(residuals, bins=30, kde=True, color='salmon')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Residuals\n",
    "\n",
    "The histogram above visualizes the distribution of residuals from a regression model. Here's what it indicates:\n",
    "\n",
    "1. **Residuals on X-Axis:** The horizontal axis represents the residuals, which are the differences between observed values and predicted values by the model.\n",
    "2. **Frequency on Y-Axis:** The vertical axis represents the frequency of residuals, showing how often residuals of a certain value occur.\n",
    "3. **Shape of Distribution:**\n",
    "   - The residuals appear to be approximately centered around 0, indicating that the model's predictions are, on average, accurate.\n",
    "   - The presence of a smooth kernel density estimation (KDE) curve provides an additional visualization of the residual distribution.\n",
    "   - Most residuals are close to 0, which suggests that the model performs well for most observations.\n",
    "4. **Spread and Outliers:**\n",
    "   - The residuals exhibit a slight right-skew (a longer tail to the right), meaning a small number of instances where the model underestimates or overestimates by a larger margin.\n",
    "   - There are some extreme residuals on both ends (e.g., near -10 and +30), which may represent outliers.\n",
    "\n",
    "### Interpretation\n",
    "This plot helps assess the assumptions of regression modeling:\n",
    "- **Normality:** The residuals are roughly symmetric around 0, which is a good indication of normality but not perfect.\n",
    "- **Homoscedasticity:** The spread of residuals appears consistent for most values, though some larger residuals exist, suggesting potential issues with heteroscedasticity.\n",
    "\n",
    "Overall, this plot provides a good diagnostic for evaluating how well the regression model fits the data and identifying any unusual patterns in the residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Feature Importance Using SHAP\n",
    "\n",
    "While traditional feature importance metrics provided by models like XGBoost offer insights into which features contribute most to the predictions, SHAP (SHapley Additive exPlanations) provides a more detailed and interpretable understanding. SHAP values account for feature interactions and offer both global and local interpretability, making them invaluable for model explanations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Extracting the preprocessed training data\n",
    "X_train_preprocessed = pipeline.named_steps['preprocessor'].transform(X_train)\n",
    "\n",
    "# Retrieving feature names after One-Hot Encoding\n",
    "onehot_features = pipeline.named_steps['preprocessor'].named_transformers_['onehot'].get_feature_names_out(categorical_features)\n",
    "all_features = list(onehot_features) + numeric_features\n",
    "\n",
    "# Converting to DataFrame for SHAP\n",
    "X_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, columns=all_features)\n",
    "\n",
    "# Creating the SHAP explainer\n",
    "explainer = shap.TreeExplainer(pipeline.named_steps['model'])\n",
    "\n",
    "# Calculating SHAP values for the training set\n",
    "shap_values = explainer.shap_values(X_train_preprocessed_df)\n",
    "\n",
    "# Bar plot for global feature importance\n",
    "plt.figure(figsize=(12,8))\n",
    "shap.summary_plot(shap_values, X_train_preprocessed_df, plot_type=\"bar\", feature_names=all_features, show=False)\n",
    "plt.title('SHAP Feature Importance (Bar Plot)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed summary plot\n",
    "shap.summary_plot(shap_values, X_train_preprocessed_df, feature_names=all_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Analysis Outcomes\n",
    "\n",
    "### 1. Bar Plot: SHAP Feature Importance\n",
    "The first plot illustrates the global feature importance using SHAP values. Key takeaways:\n",
    "- **Attendance** has the highest contribution to the model predictions, indicating its significant influence on the target outcome.\n",
    "- **Hours_Studied** and **Previous_Scores** are also critical predictors, showing that academic efforts and past performance strongly affect the target variable.\n",
    "- **Tutoring_Sessions** and **Access_to_Resources_High** play a moderate role in the model's predictions.\n",
    "- Features like **Motivation_Level_High**, **Parental_Involvement_High**, and **Peer_Influence_Positive** demonstrate smaller but still notable contributions.\n",
    "- Features such as **Peer_Influence_Negative**, **Parental_Education_Level_Postgraduate**, and **Motivation_Level_Low** exhibit minimal importance.\n",
    "\n",
    "### 2. Summary Plot: SHAP Value Distribution\n",
    "The second plot provides insights into both the magnitude and direction of the feature impacts:\n",
    "- **Attendance**: High SHAP values for attendance (blue points moving to the right) positively impact the model, while low attendance (red points to the left) negatively influences the prediction.\n",
    "- **Hours_Studied**: More hours studied (red points) drive the model output positively, while fewer hours (blue points) contribute negatively.\n",
    "- **Previous_Scores**: High scores in the past (red points) lead to better predictions, and low past scores (blue points) reduce the target output.\n",
    "- **Tutoring_Sessions**: More tutoring sessions (red) positively affect the model output, while fewer sessions (blue) have a negative impact.\n",
    "- **Access_to_Resources_High**: Students with high resource access (red) perform better, whereas those with low access (blue) have reduced output.\n",
    "\n",
    "### Feature Value Colors:\n",
    "- **Blue** indicates low feature values, and **red** indicates high feature values.\n",
    "- The distribution highlights that positive SHAP values push the model predictions higher, while negative values lower them.\n",
    "\n",
    "### Implications:\n",
    "- Strategies to improve outcomes should focus on increasing attendance, study hours, and access to resources.\n",
    "- Targeted interventions like tutoring and enhancing parental involvement can significantly boost performance.\n",
    "- Identifying and addressing negative influences, such as low motivation or poor peer influence, can also improve the target outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Performance on Subgroups\n",
    "\n",
    "Evaluating model performance across different categories helps in identifying potential biases or areas where the model performs inconsistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding predictions to the test set\n",
    "test_data = X_test.copy()\n",
    "test_data['Actual_Exam_Score'] = y_test\n",
    "test_data['Predicted_Exam_Score'] = y_pred\n",
    "test_data['Residual'] = residuals\n",
    "\n",
    "# Function to evaluate performance by subgroup\n",
    "def evaluate_subgroup(subgroup, subgroup_name):\n",
    "    subgroup_data = test_data[test_data[subgroup] == subgroup_name]\n",
    "    if len(subgroup_data) == 0:\n",
    "        print(f\"**{subgroup} = {subgroup_name}**\")\n",
    "        print(\"No data available for this subgroup.\\n\")\n",
    "        return\n",
    "    mae = mean_absolute_error(subgroup_data['Actual_Exam_Score'], subgroup_data['Predicted_Exam_Score'])\n",
    "    mse = mean_squared_error(subgroup_data['Actual_Exam_Score'], subgroup_data['Predicted_Exam_Score'])\n",
    "    r2 = r2_score(subgroup_data['Actual_Exam_Score'], subgroup_data['Predicted_Exam_Score'])\n",
    "    print(f\"**{subgroup} = {subgroup_name}**\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"R² Score: {r2:.2f}\\n\")\n",
    "\n",
    "# Example: Evaluating by 'Parental_Involvement'\n",
    "print(\"Model Performance by Parental Involvement Level:\")\n",
    "for level in data_cleaned['Parental_Involvement'].unique():\n",
    "    evaluate_subgroup('Parental_Involvement', level)\n",
    "\n",
    "# Example: Evaluating by 'School_Type'\n",
    "print(\"Model Performance by School Type:\")\n",
    "for school_type in data_cleaned['School_Type'].unique():\n",
    "    evaluate_subgroup('School_Type', school_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance by Subgroups\n",
    "\n",
    "The following tables summarize the model's performance metrics for different subgroups based on key categorical features:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Model Performance by Parental Involvement Level**\n",
    "This analysis evaluates the model's performance for different levels of parental involvement:\n",
    "\n",
    "| Parental Involvement Level | MAE  | MSE  | R² Score |\n",
    "|----------------------------|------|------|----------|\n",
    "| **Low**                   | 1.30 | 9.77 | 0.43     |\n",
    "| **Medium**                | 0.85 | 2.70 | 0.79     |\n",
    "| **High**                  | 1.19 | 8.54 | 0.51     |\n",
    "\n",
    "- **MAE (Mean Absolute Error):** Indicates the average magnitude of errors in predictions.\n",
    "- **MSE (Mean Squared Error):** Penalizes larger errors by squaring the residuals.\n",
    "- **R² Score:** Reflects how well the model explains variance in the actual scores. Higher values indicate better performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Model Performance by School Type**\n",
    "This analysis evaluates the model's performance for different types of schools:\n",
    "\n",
    "| School Type | MAE  | MSE  | R² Score |\n",
    "|-------------|------|------|----------|\n",
    "| **Public**  | 1.04 | 5.26 | 0.63     |\n",
    "| **Private** | 1.05 | 7.38 | 0.60     |\n",
    "\n",
    "---\n",
    "\n",
    "### Insights\n",
    "- The model performs best for the **Medium** parental involvement level, with the lowest MAE and MSE, and the highest R² Score (0.79).\n",
    "- The **Public** school type achieves slightly better performance than **Private** schools, with a lower MSE and higher R² Score.\n",
    "- **Low** and **High** parental involvement levels show weaker performance, suggesting areas for potential improvement in the model's predictive ability for these subgroups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Overestimation vs. Underestimation\n",
    "\n",
    "Assessing whether the model tends to overestimate or underestimate exam scores provides insights into potential biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing predictions\n",
    "test_data['Error_Type'] = test_data['Residual'].apply(lambda x: 'Underestimation' if x < 0 else 'Overestimation')\n",
    "\n",
    "# Counting error types\n",
    "error_counts = test_data['Error_Type'].value_counts()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x=error_counts.index, y=error_counts.values, palette='coolwarm')\n",
    "plt.title('Overestimation vs. Underestimation')\n",
    "plt.xlabel('Error Type')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Calculating percentages\n",
    "total_errors = error_counts.sum()\n",
    "over_percent = (error_counts.get('Overestimation', 0) / total_errors) * 100\n",
    "under_percent = (error_counts.get('Underestimation', 0) / total_errors) * 100\n",
    "print(f\"Overestimation: {over_percent:.2f}%\")\n",
    "print(f\"Underestimation: {under_percent:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The bar plot below shows the distribution of errors categorized as **Overestimation** and **Underestimation**. This analysis provides insights into potential biases in the model's predictions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Plot Description\n",
    "- **X-Axis (Error Type):** Shows the two categories of error:\n",
    "  - **Underestimation:** The model predicted a value lower than the actual exam score.\n",
    "  - **Overestimation:** The model predicted a value higher than the actual exam score.\n",
    "- **Y-Axis (Count):** Represents the number of instances for each error type.\n",
    "- The color scheme (`coolwarm` palette) distinguishes between the two error categories for better interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "#### Observations\n",
    "- **Underestimation:** Accounts for **53.84%** of the errors, suggesting a slight tendency of the model to predict lower than the actual exam scores.\n",
    "- **Overestimation:** Represents **46.16%** of the errors, indicating that while the model occasionally overpredicts, it does so less frequently compared to underestimations.\n",
    "\n",
    "---\n",
    "\n",
    "#### Insights\n",
    "- The imbalance between underestimation and overestimation, while not drastic, could suggest a slight bias in the model. This could be due to the distribution of target values or other factors influencing the predictions.\n",
    "- The overall percentages of error types (53.84% underestimation, 46.16% overestimation) are relatively balanced, indicating that the model does not have a significant skew in its predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Model and Cleaned Data\n",
    "\n",
    "To ensure reproducibility and ease of access, the following steps save the trained pipeline and export the cleaned dataset:\n",
    "- The **cleaned dataset** is saved in the `data` folder as `StudentPerformaceFactorsClean.csv`.\n",
    "- The trained **pipeline (including preprocessing and XGBoost model)** is saved in the `models` folder as `pipeline_model.joblib`.\n",
    "\n",
    "These folders are created automatically if they do not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the cleaned data to the data folder\n",
    "cleaned_data_path = os.path.join('data', 'StudentPerformaceFactorsClean.csv')\n",
    "data_cleaned.to_csv(cleaned_data_path, index=False)\n",
    "print(f\"Cleaned dataset saved at '{cleaned_data_path}'.\")\n",
    "\n",
    "# Save the pipeline (including preprocessing and XGBoost model) to the models folder\n",
    "model_path = os.path.join('models', 'pipeline_model.joblib')\n",
    "joblib.dump(pipeline, model_path)\n",
    "print(f\"Trained pipeline saved at '{model_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "- Kaggle\n",
    "- ChatGPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
